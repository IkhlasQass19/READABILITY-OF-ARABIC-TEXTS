{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cd4624-b040-401a-8f22-9bf6093d4eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\XPS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def count_sentences_arabic(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)+1\n",
    "def analyze_arabic_text(text):\n",
    "    # Nombre de phrases dans le texte\n",
    "    sentences = count_sentences_arabic(text)\n",
    "\n",
    "    # Nombre de mots dans le texte\n",
    "    words = len(text.split())\n",
    "\n",
    "    # Nombre de caractères dans le texte\n",
    "    characters = len(text)\n",
    "\n",
    "    return sentences, words, characters\n",
    "\n",
    "def analyze_arabic_file(file_path):\n",
    "    # Lecture du contenu du fichier\n",
    "    print(file_path)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read() # lire tout le contenu du fichier et stocker le dans la variable file_content\n",
    "\n",
    "        # Analyse du texte du fichier en utilisant la fonction analyze_arabic_text\n",
    "        sentences_count, words_count, characters_count = analyze_arabic_text(file_content)\n",
    "\n",
    "        # Affichage des résultats\n",
    "        #print(\"Le nombre de phrases dans le fichier est : \", sentences_count)\n",
    "        #print(\"Le nombre de mots dans le fichier est : \", words_count)\n",
    "        #print(\"Le nombre de caractères dans le fichier est : \", characters_count)\n",
    "        return sentences_count, words_count, characters_count\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fichier introuvable : {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e83c24b-3062-490e-ac57-44da5ffc8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpype import startJVM, shutdownJVM, java\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import jpype\n",
    "from jpype import *\n",
    "\n",
    "def extraire_colonne_secondaire(texte):\n",
    "    mots = texte.split('{{')\n",
    "    colonne_secondaire = [mot.split(':')[1][:-2] if '}}' in mot else '' for mot in mots]\n",
    "    colonne_secondaire = [mot.strip('}').strip() for mot in colonne_secondaire if mot]\n",
    "    return colonne_secondaire\n",
    "\n",
    "\n",
    "def lsp(chemin_fichier):\n",
    "    #print(jpype.getDefaultJVMPath())\n",
    "    path1='c:\\\\tmp'\n",
    "    \n",
    "    if not jpype.isJVMStarted():\n",
    "        jpype.startJVM(jpype.getDefaultJVMPath() ,'-ea', classpath=[path1],convertStrings=True )\n",
    "    \n",
    "    path2= 'C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\PosTaggerAlKhalil.jar'\n",
    "    jpype.addClassPath(path2)\n",
    "    \n",
    "    path3= 'C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\ADAT-Analyzer.jar'\n",
    "    jpype.addClassPath(path3)\n",
    "    \n",
    "    #Import the Java class\n",
    "    LemmaStemma = jpype.JClass(\"net.oujda_nlp_team.ADATAnalyzer\")\n",
    "    POStag = jpype.JClass(\"net.nlp.parser.TestParserClient\")\n",
    "    \n",
    "    # Chemin vers le fichier\n",
    "    #chemin_fichier = r\"C:\\Users\\mimif\\OneDrive\\Bureau\\S3\\maz\\Ressources lisibilité\\niveaux\\1\\استئجار شقة.txt\"\n",
    "    #chemin_fichier = r\"C:\\Users\\mimif\\OneDrive\\Bureau\\S3\\maz\\Ressources lisibilité\\niveaux\\test.txt\"\n",
    "    \n",
    "    with open(chemin_fichier, encoding=\"utf8\") as file:\n",
    "        file_content = file.read()\n",
    "        \n",
    "    #Lemmatizer, type de retour String\n",
    "    lemmes = LemmaStemma.getInstance().processADATLemmatizerString(file_content)\n",
    "    \n",
    "    #Stemmer, type de retour String\n",
    "    stems = LemmaStemma.getInstance().processADATStemmerString(file_content)\n",
    "\n",
    "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    lemmes = extraire_colonne_secondaire(lemmes)\n",
    "    stems = extraire_colonne_secondaire(stems)\n",
    "\n",
    "     # Retirer les marqueurs \"{{\" et \"}}\" de chaque élément avant de les fusionner\n",
    "    lemmes = [lemme.replace('{{', '').replace('}}', '') for lemme in lemmes]\n",
    "    stems = [stem.replace('{{', '').replace('}}', '') for stem in stems]\n",
    "    # Concaténer les lemmes et les stems en une seule chaîne de caractères\n",
    "    #lemme_string = ' '.join(lemmes)\n",
    "    #stem_string = ' '.join(stems)\n",
    "    \n",
    "    return  lemmes, stems#lemme_string, stem_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4eea6b-e788-45e4-a534-c8808cc8f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jpype import startJVM, shutdownJVM, java\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import jpype\n",
    "from jpype import *\n",
    "\n",
    "\n",
    "\n",
    "def write_lemmas_to_file(lemmas_list):\n",
    "    # Chemin vers le fichier temporaire\n",
    "    file_path = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\tmp.txt\"  # Adapter le chemin si nécessaire\n",
    "    \n",
    "    # Écriture des lemmes dans le fichier temporaire\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for lemma in lemmas_list:\n",
    "            file.write(lemma + '\\n')\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def extraire_tags_POS(fichier_POS):\n",
    "    # Lire le contenu du fichier de sortie POS\n",
    "    with open(fichier_POS, 'r', encoding='utf-8') as file:\n",
    "        contenu = file.read()\n",
    "    \n",
    "    # Extraire les tags PoS de chaque {{ }} dans le fichier de sortie POS\n",
    "    tags_POS = []\n",
    "    elements = contenu.split('{{')\n",
    "    for element in elements:\n",
    "        if '|' in element and '+' in element:\n",
    "            tag = element.split('+')[1].split('|')[0]\n",
    "            tags_POS.append(tag)\n",
    "\n",
    "    # Construire une chaîne avec des espaces entre chaque tag POS\n",
    "    tags_string = ' '.join(tags_POS)\n",
    "    \n",
    "    return tags_string\n",
    "\n",
    "def POS(chemin_fichier):\n",
    "\n",
    "    #chemin_fichier = write_lemmas_to_file(lems)\n",
    "    #print(jpype.getDefaultJVMPath())\n",
    "    path1='c:\\\\tmp'\n",
    "    \n",
    "    if not jpype.isJVMStarted():\n",
    "        jpype.startJVM(jpype.getDefaultJVMPath() ,'-ea', classpath=[path1],convertStrings=True )\n",
    "\n",
    "    path3= 'C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\ADAT-Analyzer.jar'\n",
    "    jpype.addClassPath(path3)\n",
    "    \n",
    "    #Import the Java class\n",
    "    POSclass = jpype.JClass(\"net.oujda_nlp_team.ADATAnalyzer\")\n",
    "    \n",
    "    with open(chemin_fichier, encoding=\"utf8\") as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    #POSTagger, type de retour .txt file\n",
    "    POSclass.getInstance().processADATStemmerWithPOSFile(chemin_fichier, \"utf-8\", \"output_POS.txt\", \"utf-8\")\n",
    "    \n",
    "    #liste de pos tags\n",
    "    chemin_fichier = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\output_POS.txt\"\n",
    "    pos = extraire_tags_POS(chemin_fichier)\n",
    "    \n",
    "    return pos\n",
    "\n",
    "def lemmes_ambigus(lems, pos):\n",
    "    lemme_amb = []\n",
    "    for lem in lems: #set(lems):  # Parcours des lemmes uniques\n",
    "        indices = [i for i, x in enumerate(lems) if x == lem]  # Récupération des indices du lem dans lems\n",
    "        pos_tmp = pos[indices[0]]  # Premier tag POS associé au lem\n",
    "        \n",
    "        for index in indices[1:]:  # Parcours des indices restants pour le même lem\n",
    "            pos_actuel = pos[index]  # Tag POS actuel associé au lem\n",
    "            if pos_actuel != pos_tmp:  # Comparaison des tags POS\n",
    "                lemme_amb.append(lem)  # Si les tags POS sont différents, ajout à la liste des lemmes ambigus\n",
    "                break  # Sortie de la boucle pour passer au lem suivant\n",
    "\n",
    "    return lemme_amb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1704cb0e-5e39-4aed-b8c8-c47d28c406c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlever_ponctuation_arabe(fichier):\n",
    "    # Liste des symboles de ponctuation arabes\n",
    "    ponctuation_arabe = '،؍؛؟٪٫٬؞؟؛؛،۔؟٪٫٬٭٠١٢٣٤٥٦٧٨٩٪٪۔ۖۗۘۙۚۛۜ۝۞ۣ۟۠ۡۢۤۥۦۧۨ۩‘’“”•…:'\n",
    "    \n",
    "    # Lecture du fichier\n",
    "    with open(fichier, 'r', encoding='utf-8') as file:\n",
    "        contenu = file.read()\n",
    "\n",
    "    # Suppression de la ponctuation arabe\n",
    "    contenu_sans_ponctuation = ''.join(caractere for caractere in contenu if caractere not in ponctuation_arabe)\n",
    "\n",
    "    # Écriture du contenu sans ponctuation dans le même fichier\n",
    "    with open(fichier, 'w', encoding='utf-8') as file:\n",
    "        file.write(contenu_sans_ponctuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "998d76fb-bef0-450b-a612-e375a08766d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def enlever_ponctuation(fichier):\n",
    "    # Liste des symboles de ponctuation\n",
    "    ponctuation = string.punctuation\n",
    "\n",
    "    # Lecture du fichier\n",
    "    with open(fichier, 'r', encoding='utf-8') as file:\n",
    "        contenu = file.read()\n",
    "\n",
    "    # Suppression de la ponctuation\n",
    "    '''When using join() in Python, the string specified before the join() method acts as a separator between the elements being joined.\n",
    "     indicates that all the characters that pass the condition (i.e., \n",
    "     those not in ponctuation) will be concatenated directly without any spaces or separators between them, forming a single string.    \n",
    "    '''\n",
    "    contenu_sans_ponctuation = ''.join(caractere for caractere in contenu if caractere not in ponctuation) # '' emptycaract not space caract\n",
    "\n",
    "    # Écriture du contenu sans ponctuation dans le même fichier\n",
    "    with open(fichier, 'w', encoding='utf-8') as file:\n",
    "        file.write(contenu_sans_ponctuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a1d34c1-6e7b-408b-951e-a4155d003a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_element(lemmes, stems):\n",
    "  \n",
    "    # Calcul du nombre de lemmes générés\n",
    "    nombre_lemmes = len(lemmes)\n",
    "\n",
    "    # Calcul du nombre de stems\n",
    "    nombre_stems = len(stems)\n",
    "    \n",
    "    # Création d'un ensemble (set) pour obtenir les lemmes distincts\n",
    "    #Lorsque nous convertissons une liste (ou toute autre structure itérable) en un ensemble en utilisant set(), \n",
    "        #Python élimine automatiquement les doublons, ne laissant que les éléments uniques.\n",
    "    lemmes_distincts = set(lemmes)\n",
    "    \n",
    "    # Calcul du nombre de lemmes distincts\n",
    "    nombre_lemmes_distincts = len(lemmes_distincts)\n",
    "    \n",
    "    # Calcul du nombre de lemmes fréquents (apparaissant plus d'une fois)\n",
    "    lemmes_freq = {}\n",
    "    for lemme in lemmes:\n",
    "        if lemme in lemmes_freq:\n",
    "            lemmes_freq[lemme] += 1\n",
    "        else:\n",
    "            lemmes_freq[lemme] = 1\n",
    "\n",
    "    nombre_lemmes_freq = sum(1 for lemme, freq in lemmes_freq.items() if freq > 1)\n",
    "    \n",
    "    return nombre_lemmes, nombre_lemmes_distincts, nombre_lemmes_freq, nombre_stems \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f4488b-3ef7-487b-b5cd-60e1dffdf7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xlsxwriter\n",
    "\n",
    "# Chemin du répertoire principal que vous souhaitez parcourir\n",
    "chemin_repertoire = r\"C:\\Users\\mimif\\OneDrive\\Bureau\\S3\\maz\\Ressources lisibilité\\niveaux\"\n",
    "\n",
    "# Chemin du fichier xlsx où vous souhaitez stocker les données\n",
    "chemin_fichier = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\res.xlsx\"\n",
    "workbook = xlsxwriter.Workbook(chemin_fichier)\n",
    "\n",
    "# The workbook object is then used to add new \n",
    "# worksheet via the add_worksheet() method.\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "# Écriture des en-têtes\n",
    "worksheet.write('A1', 'Nom du fichier')\n",
    "worksheet.write('B1', 'Contenu')\n",
    "worksheet.write('C1', 'sentences_count')\n",
    "worksheet.write('D1', 'words_count')\n",
    "worksheet.write('E1', 'characters_count')\n",
    "worksheet.write('F1', 'lemmes')\n",
    "worksheet.write('G1', 'lemmes_ambigus')\n",
    "worksheet.write('H1', 'nombre_lemmes')\n",
    "worksheet.write('I1', 'nombre_lemmes_distincts')\n",
    "worksheet.write('J1', 'nombre_lemmes_ambigus')\n",
    "worksheet.write('K1', 'nombre_lemmes_freq')\n",
    "worksheet.write('L1', 'stems')\n",
    "worksheet.write('M1', 'nombre_stems')\n",
    "worksheet.write('N1', 'POS')\n",
    "worksheet.write('O1', 'classe')\n",
    "ligne = 1  # Pour commencer à écrire à partir de la deuxième ligne (après les en-têtes)\n",
    "# Parcours récursif des fichiers et sous-répertoires\n",
    "for repertoire_actuel, sous_repertoires, fichiers in os.walk(chemin_repertoire):\n",
    "    # Pour chaque fichier trouvé dans le répertoire actuel\n",
    "    for nom_fichier in fichiers:\n",
    "        # Afficher le nom du sous-répertoire et le nom du fichier\n",
    "        #os.path.basename(repertoire_actuel) pour avoir seulement le nom du sous rep sans le chemin absolu \n",
    "        #print(f\"Sous-répertoire : {os.path.basename(repertoire_actuel)}, Fichier : {nom_fichier}\")\n",
    "        classe = os.path.basename(repertoire_actuel)\n",
    "        path = repertoire_actuel + \"\\\\\" + nom_fichier\n",
    "\n",
    "        # Calcul nbr de phrases, mots et caracts\n",
    "        sentences_count, words_count, characters_count = analyze_arabic_file(path)\n",
    "        # Enlever la ponctuation\n",
    "        enlever_ponctuation_arabe(path)\n",
    "        enlever_ponctuation(path)\n",
    "        # Extraction de lemmes et de stems\n",
    "        lemmes, stems = lsp(path)\n",
    "        #liste de posTagger\n",
    "        write_lemmas_to_file(lemmes)\n",
    "        file_path = r\"C:\\Users\\mimif\\OneDrive\\Bureau\\S3\\maz\\Ressources lisibilité\\tmp.txt\"\n",
    "        pos = POS(file_path)\n",
    "        # calcul nombre de lemme, lemmeDinstinct, lemmeFreq, et nombre de stem\n",
    "        nombre_lemmes, nombre_lemmes_distincts, nombre_lemmes_freq, nombre_stems = count_element(lemmes, stems)\n",
    "        #les lemmes ambigus\n",
    "        lem_amb = lemmes_ambigus(lemmes, pos)\n",
    "        #le nombre de lemmes ambigus\n",
    "        nombre_lemmes_ambigus = len(lem_amb)\n",
    "        # Construire une chaîne avec des espaces entre chaque tag POS\n",
    "        lemmes = ' '.join(lemmes)\n",
    "        lem_amb = ' '.join(lem_amb)\n",
    "        stems = ' '.join(stems)\n",
    "        \n",
    "        #print(path)\n",
    "        with open(path, encoding=\"utf8\") as file:\n",
    "            contenu = file.read()\n",
    "        # Écrire les données dans le fichier Excel\n",
    "        worksheet.write(ligne, 0, nom_fichier)  # Colonne A\n",
    "        worksheet.write(ligne, 1, contenu)\n",
    "        worksheet.write(ligne, 2, sentences_count)\n",
    "        worksheet.write(ligne, 3, words_count)\n",
    "        worksheet.write(ligne, 4, characters_count)\n",
    "        worksheet.write(ligne, 5, lemmes)\n",
    "        worksheet.write(ligne, 6, lem_amb)\n",
    "        worksheet.write(ligne, 7, nombre_lemmes)\n",
    "        worksheet.write(ligne, 8, nombre_lemmes_ambigus)\n",
    "        worksheet.write(ligne, 9, nombre_lemmes_distincts)\n",
    "        worksheet.write(ligne, 10, nombre_lemmes_freq)\n",
    "        worksheet.write(ligne, 11, stems)\n",
    "        worksheet.write(ligne, 12, nombre_stems)  \n",
    "        worksheet.write(ligne, 13, pos) \n",
    "        worksheet.write(ligne, 14, classe)  # Colonne L\n",
    "        ligne += 1  # Passage à la ligne suivante\n",
    "        \n",
    "# Fermeture du classeur\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9257b424-4648-4920-8501-1d8e32d8d68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\fichier.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Chemin du fichier à traiter\n",
    "chemin_fichier = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\fichier.txt\"\n",
    "\n",
    "# Calcul nbr de phrases, mots et caracts\n",
    "sentences_count, words_count, characters_count = analyze_arabic_file(chemin_fichier)\n",
    "# Enlever la ponctuation\n",
    "enlever_ponctuation_arabe(chemin_fichier)\n",
    "enlever_ponctuation(chemin_fichier)\n",
    "# Extraction de lemmes et de stems\n",
    "lemmes, stems = lsp(chemin_fichier)\n",
    "#liste de posTagger\n",
    "write_lemmas_to_file(lemmes)\n",
    "file_path = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\tmp.txt\"\n",
    "pos = POS(file_path)\n",
    "# calcul nombre de lemme, lemmeDinstinct, lemmeFreq, et nombre de stem\n",
    "nombre_lemmes, nombre_lemmes_distincts, nombre_lemmes_freq, nombre_stems = count_element(lemmes, stems)\n",
    "#les lemmes ambigus\n",
    "lem_amb = lemmes_ambigus(lemmes, pos)\n",
    "#le nombre de lemmes ambigus\n",
    "nombre_lemmes_ambigus = len(lem_amb)\n",
    "# Construire une chaîne avec des espaces entre chaque tag POS\n",
    "lemmes = ' '.join(lemmes)\n",
    "lem_amb = ' '.join(lem_amb)\n",
    "stems = ' '.join(stems)\n",
    "\n",
    "with open(chemin_fichier, encoding=\"utf8\") as file:\n",
    "    contenu = file.read()\n",
    "\n",
    "# Création d'un DataFrame avec les données\n",
    "df = pd.DataFrame({\n",
    "    'Nom du fichier': [os.path.basename(chemin_fichier)],\n",
    "    'Contenu': [contenu],\n",
    "    'SL1': [sentences_count],\n",
    "    'WL1': [words_count],\n",
    "    'WL5': [characters_count],\n",
    "    'lemmes': [lemmes],\n",
    "    'lemmes_ambigus': [lem_amb],\n",
    "    'nombre_lemmes': [nombre_lemmes],\n",
    "    'VL1': [nombre_lemmes_distincts],\n",
    "    'AMb1': [nombre_lemmes_ambigus],\n",
    "    'VL2': [nombre_lemmes_freq],\n",
    "    'stems': [stems],\n",
    "    'WL4': [nombre_stems],\n",
    "    'POS': [pos],\n",
    "    'classe': 'inconu'  # Vous devez spécifier la classe pour le fichier\n",
    "})\n",
    "\n",
    "# Enregistrement du DataFrame dans un fichier CSV\n",
    "chemin_csv = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\fichier.csv\"\n",
    "df.to_csv(chemin_csv, index=False)  # Ne pas inclure l'index dans le fichier CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acefca8c-ceb9-44f4-a6be-52fd6819e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_save_file(chemin_fichier):\n",
    "    # Calcul nbr de phrases, mots et caracts\n",
    "    sentences_count, words_count, characters_count = analyze_arabic_file(chemin_fichier)\n",
    "    # Enlever la ponctuation\n",
    "    enlever_ponctuation_arabe(chemin_fichier)\n",
    "    enlever_ponctuation(chemin_fichier)\n",
    "    # Extraction de lemmes et de stems\n",
    "    lemmes, stems = lsp(chemin_fichier)\n",
    "    #liste de posTagger\n",
    "    write_lemmas_to_file(lemmes)\n",
    "    file_path = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\tmp.txt\"\n",
    "    pos = POS(file_path)\n",
    "    # calcul nombre de lemme, lemmeDinstinct, lemmeFreq, et nombre de stem\n",
    "    nombre_lemmes, nombre_lemmes_distincts, nombre_lemmes_freq, nombre_stems = count_element(lemmes, stems)\n",
    "    #les lemmes ambigus\n",
    "    lem_amb = lemmes_ambigus(lemmes, pos)\n",
    "    #le nombre de lemmes ambigus\n",
    "    nombre_lemmes_ambigus = len(lem_amb)\n",
    "    # Construire une chaîne avec des espaces entre chaque tag POS\n",
    "    lemmes = ' '.join(lemmes)\n",
    "    lem_amb = ' '.join(lem_amb)\n",
    "    stems = ' '.join(stems)\n",
    "\n",
    "    with open(chemin_fichier, encoding=\"utf8\") as file:\n",
    "        contenu = file.read()\n",
    "\n",
    "    # Création d'un DataFrame avec les données\n",
    "    df = pd.DataFrame({\n",
    "        'Nom du fichier': [os.path.basename(chemin_fichier)],\n",
    "        'Contenu': [contenu],\n",
    "        'SL1': [sentences_count],\n",
    "        'WL1': [words_count],\n",
    "        'WL5': [characters_count],\n",
    "        'lemmes': [lemmes],\n",
    "        'lemmes_ambigus': [lem_amb],\n",
    "        'nombre_lemmes': [nombre_lemmes],\n",
    "        'VL1': [nombre_lemmes_distincts],\n",
    "        'AMb1': [nombre_lemmes_ambigus],\n",
    "        'VL2': [nombre_lemmes_freq],\n",
    "        'stems': [stems],\n",
    "        'WL4': [nombre_stems],\n",
    "        'POS': [pos],\n",
    "        'classe': 'inconnu'  # Spécifiez la classe pour le fichier\n",
    "    })\n",
    "\n",
    "    # Enregistrement du DataFrame dans un fichier CSV\n",
    "    chemin_csv = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\fichier.csv\"\n",
    "    df.to_csv(chemin_csv, index=False)  # Ne pas inclure l'index dans le fichier CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dc4bf6d-e16e-4ce0-972a-9320a9bc5370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\fichier.txt\n"
     ]
    }
   ],
   "source": [
    "# Appel de la fonction avec le chemin du fichier à traiter\n",
    "chemin_fichier = r\"C:\\\\Users\\\\XPS\\\\Desktop\\\\FerhatOumaimaQassimiIkhlas\\\\fichier.txt\"\n",
    "process_and_save_file(chemin_fichier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f4ebc-9325-4611-881e-738239484046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
