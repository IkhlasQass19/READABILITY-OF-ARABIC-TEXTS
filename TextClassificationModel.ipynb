{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPhtz0FXp1PJ"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KilZhdzypmWt",
    "outputId": "f5fee123-a2d5-4705-8ee2-babd4e16fed4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\XPS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\XPS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing,metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "st = ISRIStemmer()\n",
    "nltk.download('stopwords')\n",
    "stop=stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpStRrqeq2NN"
   },
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpype import startJVM, shutdownJVM, java\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import jpype\n",
    "from jpype import *\n",
    "\n",
    "def extraire_colonne_secondaire(texte):\n",
    "    mots = texte.split('{{')\n",
    "    colonne_secondaire = [mot.split(':')[1][:-2] if '}}' in mot else '' for mot in mots]\n",
    "    colonne_secondaire = [mot.strip('}').strip() for mot in colonne_secondaire if mot]\n",
    "    return colonne_secondaire\n",
    "\n",
    "\n",
    "def steem(text):\n",
    "    #print(jpype.getDefaultJVMPath())\n",
    "    #path1='c:\\\\tmp'\n",
    "    \n",
    "    #if not jpype.isJVMStarted():\n",
    "        #jpype.startJVM(jpype.getDefaultJVMPath() ,'-ea', classpath=[path1],convertStrings=True )\n",
    "    \n",
    "  \n",
    "    \n",
    "    path3= 'C:/Users/XPS/Desktop/FerhatOumaimaQassimiIkhlas/ADAT-Analyzer.jarADAT-Analyzer.jar'\n",
    "    jpype.addClassPath(path3)\n",
    "    \n",
    "    #Import the Java class\n",
    "    LemmaStemma = jpype.JClass(\"net.oujda_nlp_team.ADATAnalyzer\")\n",
    "    #POStag = jpype.JClass(\"net.nlp.parser.TestParserClient\")\n",
    "    \n",
    "    # Chemin vers le fichier\n",
    "    #chemin_fichier = r\"C:\\Users\\mimif\\OneDrive\\Bureau\\S3\\maz\\Ressources lisibilité\\niveaux\\1\\استئجار شقة.txt\"\n",
    "    #chemin_fichier = r\"C:\\Users\\mimif\\OneDrive\\Bureau\\S3\\maz\\Ressources lisibilité\\niveaux\\test.txt\"\n",
    "    \n",
    "    #with open(chemin_fichier, encoding=\"utf8\") as file:\n",
    "        #file_content = file.read()\n",
    "        \n",
    "    #Lemmatizer, type de retour String\n",
    "    #lemmes = LemmaStemma.getInstance().processADATLemmatizerString(file_content)\n",
    "    \n",
    "    #Stemmer, type de retour String\n",
    "    stems = LemmaStemma.getInstance().processADATStemmerString(text)\n",
    "\n",
    "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    #lemmes = extraire_colonne_secondaire(lemmes)\n",
    "    stems = extraire_colonne_secondaire(stems)\n",
    "\n",
    "     # Retirer les marqueurs \"{{\" et \"}}\" de chaque élément avant de les fusionner\n",
    "    #lemmes = [lemme.replace('{{', '').replace('}}', '') for lemme in lemmes]\n",
    "    stems = [stem.replace('{{', '').replace('}}', '') for stem in stems]\n",
    "    # Concaténer les lemmes et les stems en une seule chaîne de caractères\n",
    "    #lemme_string = ' '.join(lemmes)\n",
    "    stem_string = ' '.join(stems)\n",
    "    return stem_string\n",
    "     #return  lemme_string, stem_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sXScNyctr2L8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean(text):\n",
    "  #remove all English chars \n",
    "  text = re.sub(r'\\s*[A-Za-z]\\s*', ' ' , text)\n",
    "  #remove hashtags\n",
    "  text = re.sub(\"#\", \" \", text)\n",
    "  #remove all numbers \n",
    "  text = re.sub(r'\\[0-9]*\\]',' ',text)\n",
    "  #remove duplicated chars\n",
    "  text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "  #remove :) or :(\n",
    "  text = text.replace(':)', \"\")\n",
    "  text = text.replace(':(', \"\")\n",
    "  #remove multiple exclamation\n",
    "  text = re.sub(r\"(\\!)\\1+\", ' ', text)\n",
    "  #remove multiple question marks\n",
    "  text = re.sub(r\"(\\?)\\1+\", ' ', text)\n",
    "  #remove multistop\n",
    "  text = re.sub(r\"(\\.)\\1+\", ' ', text)\n",
    "  #remove additional spaces\n",
    "  text = re.sub(r\"[\\s]+\", \" \", text)\n",
    "  text = re.sub(r\"[\\n]+\", \" \", text)\n",
    "  \n",
    "  return text\n",
    "\n",
    "def remStopWords(Text):\n",
    "  return \" \".join(word for word in Text.split() if word not in stop)\n",
    "\n",
    "def stemWords(Text):\n",
    "  return \" \".join(st.stem(word) for word in Text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8dCzqSvbsa9I"
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "\n",
    "dataFrame = pd.read_csv('res.csv')\n",
    "\n",
    "dataFrame['Contenu']=dataFrame['Contenu'].apply(lambda Text: remStopWords(str(Text)))\n",
    "dataFrame['Contenu'] = dataFrame['Contenu'].apply(lambda Text : clean(Text))\n",
    "dataFrame['Contenu']=dataFrame['Contenu'].apply(lambda Text: stemWords(Text))\n",
    "\n",
    "dataFrame.drop_duplicates(subset =\"Contenu\",keep = 'first', inplace = True)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(dataFrame['Contenu'], dataFrame['classe'],test_size=0.2,random_state=42)\n",
    "\n",
    "before_encode_valid_y = dataFrame['classe'].unique()\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "\n",
    "tfidf_vect.fit(dataFrame['Contenu'])\n",
    "\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7459GNT0q2f2"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sLlYDviBr9Hf"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label,feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y),predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w9v5N-zxstO3"
   },
   "outputs": [],
   "source": [
    "params_grid = [\n",
    "                    {'kernel': ['linear']}]\n",
    "\n",
    "clf_svm = GridSearchCV(SVC(), params_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfEhBC7Pswqu",
    "outputId": "b7c80344-56b9-4f67-a286-4366e6608d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rapport de classification :\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m report_lines \u001b[38;5;241m=\u001b[39m report\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# Exclure les lignes d'en-tête et de pied de page\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Imprimer le tableau de classification\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTableau de classification pour le noyau \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(report_lines))  \u001b[38;5;66;03m# Imprimer les lignes du tableau de classification\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kernel' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Entraîner le modèle SVM\n",
    "clf_svm.fit(xtrain_tfidf, train_y)\n",
    "\n",
    "# Prédictions sur les données de validation\n",
    "predictions = clf_svm.predict(xvalid_tfidf)\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "conf_matrix = confusion_matrix(valid_y, predictions)\n",
    "\n",
    "\n",
    "# Calculer et afficher d'autres indicateurs de performance\n",
    "print(\"\\nRapport de classification :\")\n",
    "\n",
    "    # Générer le rapport de classification\n",
    "report = classification_report(valid_y, predictions, zero_division=1)\n",
    "\n",
    "    # Extraire uniquement la partie du rapport de classification contenant le tableau\n",
    "report_lines = report.split('\\n')[:-3]  # Exclure les lignes d'en-tête et de pied de page\n",
    "\n",
    "    # Imprimer le tableau de classification\n",
    "print(f\"Tableau de classification pour le noyau '{kernel}':\")\n",
    "print('\\n'.join(report_lines))  # Imprimer les lignes du tableau de classification\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tableau de classification pour le noyau 'linear':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.77         8\n",
      "           1       0.50      1.00      0.67         4\n",
      "           2       0.84      0.80      0.82        20\n",
      "           3       0.67      0.62      0.64        13\n",
      "           4       0.50      0.56      0.53         9\n",
      "\n",
      "    accuracy                           0.70        54\n",
      "\n",
      "\n",
      "Tableau de classification pour le noyau 'rbf':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50         8\n",
      "           1       1.00      0.00      0.00         4\n",
      "           2       0.45      0.95      0.61        20\n",
      "           3       1.00      0.00      0.00        13\n",
      "           4       0.75      0.33      0.46         9\n",
      "\n",
      "    accuracy                           0.48        54\n",
      "\n",
      "\n",
      "Tableau de classification pour le noyau 'poly':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.88      0.64         8\n",
      "           1       1.00      0.00      0.00         4\n",
      "           2       0.46      0.85      0.60        20\n",
      "           3       1.00      0.00      0.00        13\n",
      "           4       0.67      0.22      0.33         9\n",
      "\n",
      "    accuracy                           0.48        54\n",
      "\n",
      "\n",
      "Tableau de classification pour le noyau 'sigmoid':\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      1.00         8\n",
      "           1       1.00      0.00      0.00         4\n",
      "           2       0.40      0.95      0.56        20\n",
      "           3       1.00      0.00      0.00        13\n",
      "           4       1.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.35        54\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "'''#read data\n",
    "dataFrame = pd.read_csv('res.csv')\n",
    "\n",
    "dataFrame['Contenu']=dataFrame['Contenu'].apply(lambda Text: remStopWords(str(Text)))\n",
    "dataFrame['Contenu'] = dataFrame['Contenu'].apply(lambda Text : clean(Text))\n",
    "dataFrame['Contenu']=dataFrame['Contenu'].apply(lambda Text: stemWords(Text))\n",
    "\n",
    "dataFrame.drop_duplicates(subset =\"Contenu\",keep = 'first', inplace = True)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(dataFrame['Contenu'], dataFrame['classe'],test_size=0.2,random_state=(50))\n",
    "\n",
    "before_encode_valid_y = dataFrame['classe'].unique()\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=50000)\n",
    "\n",
    "tfidf_vect.fit(dataFrame['Contenu'])\n",
    "\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack# Importez vos fonctions personnalisées\n",
    "\n",
    "# Lecture des données à partir du fichier CSV\n",
    "dataFrame = pd.read_csv('res.csv')\n",
    "\n",
    "# Prétraitement du contenu\n",
    "dataFrame['Contenu'] = dataFrame['Contenu'].apply(lambda Text: remStopWords(str(Text)))\n",
    "dataFrame['Contenu'] = dataFrame['Contenu'].apply(lambda Text: clean(Text))\n",
    "dataFrame['Contenu'] = dataFrame['Contenu'].apply(lambda Text: stemWords(Text))\n",
    "\n",
    "# Suppression des doublons\n",
    "dataFrame.drop_duplicates(subset=\"Contenu\", keep='first', inplace=True)\n",
    "\n",
    "# Division en ensembles d'entraînement et de validation\n",
    "train_data, valid_data = train_test_split(dataFrame, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encodage des étiquettes de classe\n",
    "encoder = LabelEncoder()\n",
    "train_data['classe'] = encoder.fit_transform(train_data['classe'])\n",
    "valid_data['classe'] = encoder.transform(valid_data['classe'])\n",
    "\n",
    "# Extraction des caractéristiques TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=50000)\n",
    "tfidf_vect.fit(dataFrame['Contenu'])\n",
    "\n",
    "xtrain_tfidf = tfidf_vect.transform(train_data['Contenu'])\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_data['Contenu'])\n",
    "\n",
    "# Concaténation des nouvelles caractéristiques avec les caractéristiques TF-IDF\n",
    "nouvelles_caracteristiques_train = train_data[['SL1', 'WL1', 'WL4', 'WL5', 'AMb1', 'VL1', 'VL2']].values\n",
    "nouvelles_caracteristiques_valid = valid_data[['SL1', 'WL1', 'WL4', 'WL5', 'AMb1', 'VL1', 'VL2']].values\n",
    "xtrain_combined = hstack((xtrain_tfidf, nouvelles_caracteristiques_train))\n",
    "#print(xtrain_combined)\n",
    "xvalid_combined = hstack((xvalid_tfidf, nouvelles_caracteristiques_valid))\n",
    "\n",
    "\n",
    "\n",
    "# Liste des noyaux SVM à tester\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "for kernel in kernels:\n",
    "    # Initialiser le modèle SVM avec le noyau actuel\n",
    "    svm_model = SVC(kernel=kernel)\n",
    "\n",
    "    # Entraîner le modèle SVM\n",
    "    svm_model.fit(xtrain_combined, train_data['classe'])\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    predictions = svm_model.predict(xvalid_combined)\n",
    "\n",
    "    # Générer le rapport de classification\n",
    "    report = classification_report(valid_data['classe'], predictions, zero_division=1)\n",
    "\n",
    "    # Extraire uniquement la partie du rapport de classification contenant le tableau\n",
    "    report_lines = report.split('\\n')[:-3]  # Exclure les lignes d'en-tête et de pied de page\n",
    "\n",
    "    # Imprimer le tableau de classification\n",
    "    print(f\"Tableau de classification pour le noyau '{kernel}':\")\n",
    "    print('\\n'.join(report_lines))  # Imprimer les lignes du tableau de classification\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modele_svm.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "import joblib\n",
    "\n",
    "# Choix du noyau SVM avec la meilleure précision\n",
    "best_kernel = 'linear'  #  'linear' a la meilleure précision, vous devez remplacer cela par le noyau avec la meilleure précision\n",
    "joblib.dump(tfidf_vect, 'tfidf_vect.joblib')\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(encoder, 'label_encoder.joblib')\n",
    "# Initialisation du modèle SVM avec le meilleur noyau\n",
    "best_svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Entraînement du modèle SVM avec les données d'entraînement combinées\n",
    "best_svm_model.fit(xtrain_combined, train_data['classe'])\n",
    "\n",
    "dump(best_svm_model, 'modele_svm.joblib')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 2 3 2 3 1 2 2 4 4 4 4 4 4 4 2 2 1 4 4 1 1 3 1 0 3 2 2 2 2 1 3 3 0 2\n",
      " 2 2 2 0 1 3 0 2 3 3 3 2 3 0 2 4 3]\n",
      "Tableau de classification pour le noyau :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.77         8\n",
      "           1       0.50      1.00      0.67         4\n",
      "           2       0.84      0.80      0.82        20\n",
      "           3       0.67      0.62      0.64        13\n",
      "           4       0.50      0.56      0.53         9\n",
      "\n",
      "    accuracy                           0.70        54\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "kernel= 'linear'\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack# Importez vos fonctions personnalisées\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Lecture des données à partir du fichier CSV\n",
    "dataFrame = pd.read_csv('res.csv')\n",
    "\n",
    "# Prétraitement du contenu\n",
    "dataFrame['Contenu'] = dataFrame['Contenu'].apply(lambda Text: remStopWords(str(Text)))\n",
    "dataFrame['Contenu'] = dataFrame['Contenu'].apply(lambda Text: clean(Text))\n",
    "dataFrame['Contenu'] = dataFrame['Contenu'].apply(lambda Text: stemWords(Text))\n",
    "\n",
    "# Suppression des doublons\n",
    "dataFrame.drop_duplicates(subset=\"Contenu\", keep='first', inplace=True)\n",
    "\n",
    "# Division en ensembles d'entraînement et de validation\n",
    "train_data, valid_data = train_test_split(dataFrame, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encodage des étiquettes de classe\n",
    "encoder = LabelEncoder()\n",
    "train_data['classe'] = encoder.fit_transform(train_data['classe'])\n",
    "valid_data['classe'] = encoder.transform(valid_data['classe'])\n",
    "\n",
    "# Extraction des caractéristiques TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "tfidf_vect.fit(dataFrame['Contenu'])\n",
    "\n",
    "xtrain_tfidf = tfidf_vect.transform(train_data['Contenu'])\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_data['Contenu'])\n",
    "\n",
    "# Concaténation des nouvelles caractéristiques avec les caractéristiques TF-IDF\n",
    "nouvelles_caracteristiques_train = train_data[['SL1', 'WL1', 'WL4', 'WL5', 'AMb1', 'VL1', 'VL2']].values\n",
    "nouvelles_caracteristiques_valid = valid_data[['SL1', 'WL1', 'WL4', 'WL5', 'AMb1', 'VL1', 'VL2']].values\n",
    "xtrain_combined = hstack((xtrain_tfidf, nouvelles_caracteristiques_train))\n",
    "#print(xtrain_combined)\n",
    "xvalid_combined = hstack((xvalid_tfidf, nouvelles_caracteristiques_valid))\n",
    "\n",
    "\n",
    "\n",
    "    # Initialiser le modèle SVM avec le noyau actuel\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "    # Entraîner le modèle SVM\n",
    "svm_model.fit(xtrain_combined, train_data['classe'])\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "predictions = svm_model.predict(xvalid_combined)\n",
    "print(predictions)\n",
    "    # Générer le rapport de classification\n",
    "report = classification_report(valid_data['classe'], predictions, zero_division=1)\n",
    "\n",
    "    # Extraire uniquement la partie du rapport de classification contenant le tableau\n",
    "report_lines = report.split('\\n')[:-3]  # Exclure les lignes d'en-tête et de pied de page\n",
    "\n",
    "    # Imprimer le tableau de classification\n",
    "print(f\"Tableau de classification pour le noyau :\")\n",
    "print('\\n'.join(report_lines))  # Imprimer les lignes du tableau de classification\n",
    "\n",
    "print(\"\\n\")\n",
    "# Enregistrement du modèle SVM avec le noyau linéaire\n",
    "with open('modele_svm_linear.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack# Importez vos fonctions personnalisées\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "# Charger le modèle SVM sauvegardé\n",
    "svm_model = joblib.load('modele_svm.joblib')\n",
    "\n",
    "# Charger les données à partir du fichier CSV\n",
    "dataFrame_new = pd.read_csv('fichier.csv')\n",
    "\n",
    "# Prétraitement du contenu des nouvelles données\n",
    "dataFrame_new['Contenu'] = dataFrame_new['Contenu'].apply(lambda Text: remStopWords(str(Text)))\n",
    "dataFrame_new['Contenu'] = dataFrame_new['Contenu'].apply(lambda Text: clean(Text))\n",
    "dataFrame_new['Contenu'] = dataFrame_new['Contenu'].apply(lambda Text: stemWords(Text))\n",
    "# Load the TF-IDF vectorizer\n",
    "tfidf_vect = joblib.load('tfidf_vect.joblib')\n",
    "\n",
    "# Transform the new data with the TF-IDF vectorizer\n",
    "xvalid_tfidf_new = tfidf_vect.transform(dataFrame_new['Contenu'] )\n",
    "\n",
    "# Extraction des caractéristiques TF-IDF pour les nouvelles données\n",
    "#tfidf_vect = TfidfVectorizer()\n",
    "#xvalid_tfidf_new = tfidf_vect.transform(dataFrame_new['Contenu'])\n",
    "\n",
    "# Concaténation des nouvelles caractéristiques avec les caractéristiques TF-IDF\n",
    "nouvelles_caracteristiques_new = dataFrame_new[['SL1', 'WL1', 'WL4', 'WL5', 'AMb1', 'VL1', 'VL2']].values\n",
    "xvalid_combined_new = hstack((xvalid_tfidf_new, nouvelles_caracteristiques_new))\n",
    "\n",
    "# Faire des prédictions sur les nouvelles données\n",
    "predictions_new = svm_model.predict(xvalid_combined_new)\n",
    "\n",
    "# Afficher les prédictions\n",
    "print(predictions_new)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMXwzE6MJZXzAQk0S5fTBwi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1QvvWfhPl_SeUW18_wut6Zg6vJHlTUHt0",
   "name": "TextClassificationModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
